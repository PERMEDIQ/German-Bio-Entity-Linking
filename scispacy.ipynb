{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "PROCESSED  = Path('data/processed')\n",
    "(PROCESSED / \"kbs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW = Path('data/raw')\n",
    "RAW.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare KB for scispacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: UMLS_SapBERT\n",
    "\n",
    "UMLS filtered by WiKIMed Entries (Same as SAPBERT paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "cui_aliases = defaultdict(list)\n",
    "\n",
    "# taken from sapbert repo\n",
    "with open(\"data/umls_onto_all_lang_cased_wikimed_only_399931.txt\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        cui, name = line.strip(\"\\n\").split(\"||\")\n",
    "        cui_aliases[cui].append(name)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file contains duplicates. Removing duplicated does not change metric score.\n",
    "# cui_aliases = {i: list(set(j)) for i, j in cui_aliases.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = []\n",
    "for cui, aliases, in cui_aliases.items():\n",
    "    kb.append({\"concept_id\": cui, \"aliases\": aliases[1:], \"canonical_name\": aliases[0], \"definition\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(PROCESSED / \"kbs\"/ \"kb_from_sapbert.jsonl\", 'w', encoding=\"utf-8\") as outfile:\n",
    "    for entry in kb:\n",
    "        json.dump(entry, outfile, ensure_ascii=False)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 UMLS_Wikidata\n",
    "\n",
    "WIKIDATA using SPARQL\n",
    "\n",
    "The step is optional. UMLS_Wikidata KB can be downloaded as mentioned in README and processed as per step 1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install qwikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the QIDs which have CUI and save as \"qids_with_cui.csv\" file.\n",
    "\n",
    "https://w.wiki/8Fkw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from qwikidata.entity import WikidataItem\n",
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "from pprint import pprint\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def prepare_and_dump(qid, q_dict):\n",
    "    q_item = WikidataItem(q_dict)\n",
    "    aliases = q_item.get_aliases(\"de\")\n",
    "\n",
    "    \n",
    "    cuis = [\n",
    "        i._claim_dict[\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "        for i in q_item.get_claim_group(\"P2892\")\n",
    "    ]\n",
    "   \n",
    "\n",
    "    sample = {\n",
    "        \"qid\": qid,\n",
    "        \"label\": q_item.get_label(\"de\"),\n",
    "        \"description\": q_item.get_description(\"de\"),\n",
    "        \"cui\": cuis,\n",
    "        \"aliases\": aliases,\n",
    "    }\n",
    "\n",
    "    json.dump(sample, output, ensure_ascii=False)\n",
    "    output.write(\"\\n\")\n",
    "\n",
    "\n",
    "output = open(RAW / \"kbs\"/ \"qids_with_cui_output.jsonl\", \"w\", encoding=\"utf-8\")\n",
    "workers = 100 \n",
    "nloops = 731418 / workers # no of samples / no of workers (used for loop tqdm)\n",
    "\n",
    "with open(RAW / \"kbs\" /\"qids_with_cui.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\",\")\n",
    "    next(reader)\n",
    "\n",
    "    for batch in tqdm(iter(lambda: list(islice(reader, workers)), []), total=nloops):\n",
    "        batch = [qid[0].split(\"/\")[-1] for qid in batch]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as pool:\n",
    "            results = list(pool.map(get_entity_dict_from_api, batch))\n",
    "\n",
    "        for qid, q_dict in zip(batch, results):\n",
    "            prepare_and_dump(qid, q_dict)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = []\n",
    "with open(RAW / \"kbs\"/ \"qids_with_cui_output.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "\n",
    "        entry = json.loads(line)\n",
    "        for cui in set(entry[\"cui\"]):\n",
    "            # if entry[\"label\"]:\n",
    "                kb.append({\"concept_id\": cui, \"aliases\": entry[\"aliases\"], \"canonical_name\": entry[\"label\"], \"definition\": entry[\"description\"]})\n",
    "\n",
    "\n",
    "\n",
    "with open(PROCESSED / \"kbs\" / 'kb_from_wikidata_sparql.jsonl', 'w', encoding=\"utf-8\") as outfile:\n",
    "    for entry in kb:\n",
    "        json.dump(entry, outfile, ensure_ascii=False)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Once the KB is prepared, artifacts can be prepared using the following command from the root of project. It will make ANN index for the KB. \n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "python scripts/create_linker.py --kb_path \"data/processed/kbs/kb_from_sapbert.jsonl\" --output_path \"artifacts/sapbert\"\n",
    "```\n",
    "\n",
    "OR \n",
    "\n",
    "```bash\n",
    "python scripts/create_linker.py --kb_path \"data/processed/kbs/kb_from_wikidata_sparql.jsonl\" --output_path \"artifacts/sparql\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Entity Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "\n",
    "from scispacy.linking import *\n",
    "\n",
    "from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES\n",
    "from scispacy.candidate_generation import (\n",
    "    CandidateGenerator,\n",
    "    LinkerPaths\n",
    ")\n",
    "\n",
    "from scispacy.linking_utils import KnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for UMLS KB \n",
    " \n",
    "CustomLinkerPaths_2020AA = LinkerPaths(\n",
    "    ann_index=\"artifacts/sapbert/nmslib_index.bin\",  # noqa\n",
    "    tfidf_vectorizer=\"artifacts/sapbert/tfidf_vectorizer.joblib\",  # noqa\n",
    "    tfidf_vectors=\"artifacts/sapbert/tfidf_vectors_sparse.npz\",  # noqa\n",
    "    concept_aliases_list=\"artifacts/sapbert/concept_aliases.json\",  # noqa\n",
    ")\n",
    "\n",
    "KB_PATH = str(PROCESSED / \"kbs\" / \"kb_from_sapbert.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment for WIKIdata Sparql KB \n",
    "\n",
    "# CustomLinkerPaths_2020AA = LinkerPaths(\n",
    "#     ann_index=\"artifacts/sparql/nmslib_index.bin\",  # noqa\n",
    "#     tfidf_vectorizer=\"artifacts/sparql/tfidf_vectorizer.joblib\",  # noqa\n",
    "#     tfidf_vectors=\"artifacts/sparql/tfidf_vectors_sparse.npz\",  # noqa\n",
    "#     concept_aliases_list=\"artifacts/sparql/concept_aliases.json\",  # noqa\n",
    "# )\n",
    "\n",
    "# KB_PATH = str(PROCESSED / \"kbs\" / \"kb_from_wikidata_sparql.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UMLS2020KnowledgeBase(KnowledgeBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str = KB_PATH\n",
    "    ):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "DEFAULT_PATHS[\"umls2020\"] = CustomLinkerPaths_2020AA\n",
    "DEFAULT_KNOWLEDGE_BASES[\"umls2020\"] = UMLS2020KnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scispacy_linker = CandidateGenerator(name=\"umls2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# !{sys.executable} -m pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_sm-0.5.3.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_sci_sm\")\n",
    "# nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": False, \"linker_name\": \"umls2020\"} )\n",
    "\n",
    "\n",
    "# doc = nlp(\"Spinal and bulbar muscular atrophy (SBMA) is an \\\n",
    "#            inherited motor neuron disease caused by the expansion \\\n",
    "#            of a polyglutamine tract within the androgen receptor (AR). \\\n",
    "#            SBMA can be caused by this easily.\")\n",
    "\n",
    "\n",
    "# # Let's look at a random entity!\n",
    "# entity = doc.ents[0]\n",
    "\n",
    "# print(\"Name: \", entity)\n",
    "\n",
    "# # Each entity is linked to UMLS with a score\n",
    "# # (currently just char-3gram matching).\n",
    "# linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "# for umls_ent in entity._.kb_ents:\n",
    "    \n",
    "#     print(linker.kb.cui_to_entity[umls_ent[0]])\n",
    "#     print(\"=================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query dataset:  XLBEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLBEL data from SAPBERT repo\n",
    "test_queries = []\n",
    "with open(\"data/de_1k_test_query.txt\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        cui, name = line.strip(\"\\n\").split(\"||\")\n",
    "        test_queries.append((cui, name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility for evaluation metric\n",
    "def check_label(golden_cui:str , predicted_cuis:list, k:int ):\n",
    "    \"\"\"\n",
    "    Some composite annotation didn't consider orders\n",
    "    So, return True if any cui is matched within composite cui (or single cui)\n",
    "    Otherwise, return False\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for predicted_cui in predicted_cuis[:k]:\n",
    "        ans = len(set(predicted_cui.split(\"|\")).intersection(set(golden_cui.split(\"|\")))) > 0\n",
    "        result.append(ans)\n",
    "    # print(k)\n",
    "    # print(result)\n",
    "\n",
    "    return any(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_and_evaluate(test_queries, topk):\n",
    "    total_entities = 0\n",
    "    correct_at_1 = 0\n",
    "    correct_at_2 = 0\n",
    "    correct_at_5 = 0\n",
    "    correct_at_40 = 0\n",
    "    correct_at_60 = 0\n",
    "    correct_at_80 = 0\n",
    "    correct_at_100 = 0\n",
    "    \n",
    "    for label, text_span in tqdm(test_queries):\n",
    "        \n",
    "        candidates = scispacy_linker([text_span], topk)[0]\n",
    "        sorted_candidates = sorted(\n",
    "            candidates, reverse=True, key=lambda x: max(x.similarities)\n",
    "        )\n",
    "        # print(len(sorted_candidates))\n",
    "        candidate_ids = [c.concept_id for c in sorted_candidates]\n",
    "        \n",
    "        if check_label(golden_cui = label , predicted_cuis= candidate_ids, k=1 ):\n",
    "            correct_at_1 += 1\n",
    "        if check_label(golden_cui = label , predicted_cuis= candidate_ids, k=2 ):\n",
    "            correct_at_2 += 1\n",
    "        if check_label(golden_cui = label , predicted_cuis= candidate_ids, k=5 ):\n",
    "            correct_at_5 += 1\n",
    "        # if label in candidate_ids[:40]:\n",
    "        #     correct_at_40 += 1\n",
    "        # if label in candidate_ids[:60]:\n",
    "        #     correct_at_60 += 1\n",
    "        # if label in candidate_ids[:80]:\n",
    "        #     correct_at_80 += 1\n",
    "        # if label in candidate_ids[:100]:\n",
    "        #     correct_at_100 += 1\n",
    "\n",
    "        total_entities += 1\n",
    "\n",
    "    print(\"Total entities: \", total_entities)\n",
    "    print(\n",
    "        \"Correct at 1: \", correct_at_1, \"Precision at 1: \", correct_at_1 / total_entities\n",
    "    )\n",
    "    print(\n",
    "        \"Correct at 2: \", correct_at_2, \"Precision at 2: \", correct_at_2 / total_entities\n",
    "    )\n",
    "    print(\n",
    "        \"Correct at 5: \",\n",
    "        correct_at_5,\n",
    "        \"Precision at 5: \",\n",
    "        correct_at_5 / total_entities,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_and_evaluate(test_queries = test_queries, topk=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query dataset: WikiMed-DE-BEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiMed_DE_BEL = RAW / \"BEL-silver-standard/WikiMed-DE-BEL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_query_data(data):\n",
    "    from collections import defaultdict\n",
    "    name_cui_map = defaultdict(set)\n",
    "    for entry in data:\n",
    "        entry_title = entry[\"title\"]\n",
    "        entry_cui = entry[\"cui\"]\n",
    "\n",
    "        if entry_title and entry_cui !=\"None\":\n",
    "            name_cui_map[entry_title].add(entry_cui)\n",
    "           \n",
    "        \n",
    "        mentions = entry[\"mentions\"]\n",
    "        for m in mentions:\n",
    "            mention_title = m[\"mention\"]\n",
    "            mention_cui = m[\"cui\"]\n",
    "\n",
    "            if mention_title and mention_cui !=\"None\":\n",
    "                name_cui_map[mention_title].add(mention_cui)\n",
    "                \n",
    "    \n",
    "    test_queries = [(f\"{'|'.join(cuis)}\",name) for name, cuis in name_cui_map.items()]\n",
    "                \n",
    "    return test_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WikiMed_DE_BEL / \"train_data_bel.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = set(prepare_query_data(data))\n",
    "len(test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_and_evaluate(test_queries = test_queries, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WikiMed_DE_BEL / \"dev_data_bel.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = set(prepare_query_data(data))\n",
    "len(test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_and_evaluate(test_queries = test_queries, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WikiMed_DE_BEL / \"test_data_bel.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = set(prepare_query_data(data))\n",
    "len(test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_and_evaluate(test_queries = test_queries, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
